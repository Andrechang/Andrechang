<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title>LSTM implementation explained - Andrechang</title>
        <!-- meta -->
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
        <meta name="generator" content="Jekyll" />
        <meta name="author" content="Andrechang" />
        <meta name="description" content="Andrechang's blog" />
        <meta name="keywords" content="" />
        <!-- atom -->
        <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
        <link rel="shortcut icon" href="/images/shortcut.jpg" type="image/x-icon" />
        <!-- font-awesome -->
        <link href="//netdna.bootstrapcdn.com/font-awesome/3.2.1/css/font-awesome.css" rel="stylesheet">
        <link href='http://fonts.useso.com/css?family=Spirax' rel='stylesheet' type='text/css'>
        <link rel="stylesheet" href="/css/syntax.css">
        <link rel="stylesheet" href="/css/main.css">

        
        
    </head>

    <body>
    <!-- <body  data-spy="scroll" data-target=".toc" data-offset="20"> -->
        <div class="head fn-clear">
            <div class="header">
                <h1 class="logo">
                    <a href=""><i class="icon-anchor"></i></a>
                </h1>
                <nav class="nav">
                    <ul>
                        
                        
                        
                        <li class="nav-item ">
                            <a href="/index.html">
                                HOME
                            </a>
                            
                        </li>
                        
                        
                        
                        <li class="nav-item ">
                            <a href="/categories.html">
                                CATEGORIES
                            </a>
                            
                        </li>
                        
                        
                        
                        <li class="nav-item ">
                            <a href="/about.html">
                                ABOUT ME
                            </a>
                            
                        </li>
                        
                        
                        
                        <li class="nav-item ">
                            <a href="/links.html">
                                LINKS
                            </a>
                            
                        </li>
                        
                    </ul>
                </nav>
                <div class="follow">
                    
                    <a href="/atom.xml" target="_blank"><i class="icon-rss"></i></a>
                    
                    <a href="https://www.facebook.com/" target="_blank"><i class="icon-facebook"></i></a>
                    
                    <a href="https://github.com/Andrechang/" target="_blank"><i class="icon-github-alt"></i></a>
                    
                </div>
            </div>
        </div>

        <div class="contain fn-clear">
            <div class="container fn-clear">
                <!-- toc -->
                <!-- 
                <div class="main-left">
                    <div id="toc"></div>
                </div>
                 -->

                <div class="main">
                    <div class="article article-post">
    <h2 class="title">LSTM implementation explained</h2>
    <div class="info">
        <span class="info-title"><i class="icon-calendar"></i> Published: </span>
        <span class="info-date">30 Aug 2015</span>
        <span class="info-title"><i class="icon-folder-open"></i> Category: </span>
        <span class="info-link"><a href="/categories.html#-ref" ></a></span>
    </div>
    <div id="toc"></div>
    <h2 id="preface">Preface</h2>
<p>For a long time I’ve been looking for a good tutorial on implementing LSTM networks.
They seemed to be complicated and I’ve never done anything with them before.
Quick googling didn’t help, as all I’ve found were some slides.</p>

<p>Fortunately, I took part in <a href="https://www.kaggle.com/c/grasp-and-lift-eeg-detection">Kaggle EEG Competition</a> and thought that it might be fun
to use LSTMs and finally learn how they work. I based <a href="https://github.com/apaszke/kaggle-grasp-and-lift">my solution</a> and this post’s code on <a href="https://github.com/karpathy/char-rnn">char-rnn</a>
by <a href="https://karpathy.github.io">Andrej Karpathy</a>,
which I highly recommend you to check out.</p>

<h3 id="rnn-misconception">RNN misconception</h3>
<p>There is one important thing that as I feel
hasn’t been emphasized strongly enough (and is the main reason why I couldn’t get myself to
do anything with RNNs). There isn’t much difference between an RNN and
feedforward network implementation. It’s the easiest to implement an RNN just
as a feedforward network with some parts of the input feeding into the middle of the stack,
and a bunch of outputs coming out from there as well. There is no magic internal state
kept in the network. It’s provided as a part of the input!</p>

<div class="images">
  <img src="assets/posts/lstm-explained/RNNvsFNN.svg" />
  <div class="label">
    The overall structure of RNNs is very similar to that of feedforward networks.
  </div>
</div>

<h3 id="lstm-refresher">LSTM refresher</h3>

<p>This section will cover only the formal definition of LSTMs. There are lots of other nice
blog posts describing in detail how can you imagine and think of these equations.</p>

<p>LSTMs have many variations,
but we’ll stick to a simple one. One cell consists of three gates (input, forget, output),
and a cell unit. Gates use a sigmoid activation, while input and cell state is often
transformed with tanh. LSTM cell can be defined with a following set of equations:</p>

<p>Gates:</p>

<script type="math/tex; mode=display">i_{t} = g(W_{xi}x_{t} + W_{hi}h_{t-1} + b_{i})</script>

<script type="math/tex; mode=display">f_{t} = g(W_{xf}x_{t} + W_{hf}h_{t-1} + b_{f})</script>

<script type="math/tex; mode=display">o_{t} = g(W_{xo}x_{t} + W_{ho}h_{t-1} + b_{o})</script>

<p>Input transform:</p>

<script type="math/tex; mode=display">c\_in_{t} = tanh(W_{xc}x_{t} + W_{hc}h_{t-1} + b_{c\_in})</script>

<p>State update:</p>

<script type="math/tex; mode=display">c_{t} = f_{t} \cdot c_{t-1} + i_{t} \cdot c\_in_{t}</script>

<script type="math/tex; mode=display">h_{t} = o_{t} \cdot tanh(c_{t})</script>

<p>It can be pictured like this:</p>

<div class="images">
  <img alt="LSTM cell diagram" src="assets/posts/lstm-explained/cell.svg" style="width: 40%" />
</div>

<p>Because of the gating mechanism the cell can keep a piece of information for long
periods of time during work and protect the gradient inside the cell from harmful changes during the training.
Vanilla LSTMs don’t have a forget gate and add unchanged cell state
during the update (it can be seen as a recurrent connection with a constant weight of 1),
what is often referred to as a Constant Error Carousel (CEC).
It’s called like that, because it solves a serious RNN training problem of vanishing and exploding gradients,
which in turn makes it possible to learn long-term relationships.</p>

<h2 id="building-your-own-lstm-layer">Building your own LSTM layer</h2>
<p>The code for this tutorial will be using Torch7.
<strong>Don’t worry if you don’t know it</strong>. I’ll explain everything, so you’ll be able
to implement the same algorithm in your favorite framework.</p>

<p>The network will be implemented as a <code class="highlighter-rouge">nngraph.gModule</code>, which basically means that we’ll define
a computation graph consisting of standard <code class="highlighter-rouge">nn</code> modules.
We will need the following layers:</p>

<ul>
  <li><code class="highlighter-rouge">nn.Identity()</code> - passes on the input (used as a placeholder for input)</li>
  <li><code class="highlighter-rouge">nn.Dropout(p)</code> - standard dropout module (drops with probability <code class="highlighter-rouge">1 - p</code>)</li>
  <li><code class="highlighter-rouge">nn.Linear(in, out)</code> - an affine transform from <code class="highlighter-rouge">in</code> dimensions to <code class="highlighter-rouge">out</code> dims</li>
  <li><code class="highlighter-rouge">nn.Narrow(dim, start, len)</code> - selects a subvector along <code class="highlighter-rouge">dim</code> dimension having <code class="highlighter-rouge">len</code> elements starting from <code class="highlighter-rouge">start</code> index</li>
  <li><code class="highlighter-rouge">nn.Sigmoid()</code> - applies sigmoid element-wise</li>
  <li><code class="highlighter-rouge">nn.Tanh()</code> - applies tanh element-wise</li>
  <li><code class="highlighter-rouge">nn.CMulTable()</code> - outputs the product of tensors in forwarded table</li>
  <li><code class="highlighter-rouge">nn.CAddTable()</code> - outputs the sum of tensors in forwarded table</li>
</ul>

<h3 id="inputs">Inputs</h3>

<p>First, let’s define the input structure. The array-like objects in lua
are called tables. This network will accept a table of tensors like the one below:</p>

<div class="images">
  <img src="assets/posts/lstm-explained/input_table.svg" alt="Input table structure" style="width: 30%;" />
</div>

<figure class="highlight"><pre><code class="language-lua" data-lang="lua"><span class="kd">local</span> <span class="n">inputs</span> <span class="o">=</span> <span class="p">{}</span>
<span class="nb">table.insert</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Identity</span><span class="p">()())</span>   <span class="c1">-- network input</span>
<span class="nb">table.insert</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Identity</span><span class="p">()())</span>   <span class="c1">-- c at time t-1</span>
<span class="nb">table.insert</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Identity</span><span class="p">()())</span>   <span class="c1">-- h at time t-1</span>
<span class="kd">local</span> <span class="n">input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="kd">local</span> <span class="n">prev_c</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="kd">local</span> <span class="n">prev_h</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span></code></pre></figure>

<p>Identity modules will just copy whatever we provide to the network into the graph.</p>

<h3 id="computing-gate-values">Computing gate values</h3>

<p>To make our implementation faster we will be applying the transformations of the whole
LSTM layer simultaneously.</p>

<figure class="highlight"><pre><code class="language-lua" data-lang="lua"><span class="kd">local</span> <span class="n">i2h</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">rnn_size</span><span class="p">)(</span><span class="n">input</span><span class="p">)</span>  <span class="c1">-- input to hidden</span>
<span class="kd">local</span> <span class="n">h2h</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">rnn_size</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">rnn_size</span><span class="p">)(</span><span class="n">prev_h</span><span class="p">)</span>   <span class="c1">-- hidden to hidden</span>
<span class="kd">local</span> <span class="n">preactivations</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CAddTable</span><span class="p">()({</span><span class="n">i2h</span><span class="p">,</span> <span class="n">h2h</span><span class="p">})</span>       <span class="c1">-- i2h + h2h</span></code></pre></figure>

<p>If you’re unfamiliar with <code class="highlighter-rouge">nngraph</code> it probably seems strange that we’re constructing
a module and already calling it once more with a graph node. What actually happens is that the
second call converts the <code class="highlighter-rouge">nn.Module</code> to <code class="highlighter-rouge">nngraph.gModule</code> and the argument specifies it’s parent in the graph.</p>

<p><code class="highlighter-rouge">preactivations</code> outputs a vector created by a linear transform of input
and previous hidden state. These are raw values which will be used to compute the
gate activations and the cell input. This vector is divided into 4 parts, each
of size <code class="highlighter-rouge">rnn_size</code>. The first will be used for in gates, second for forget gates,
third for out gates and the last one as a cell input (so the indices of respective gates
and input of a cell number \(i\) are
\(\left\{i,\ \text{rnn_size}+i,\ 2\cdot\text{rnn_size}+i,\  3\cdot\text{rnn_size}+i\right\}\)).</p>

<div class="images">
  <img src="assets/posts/lstm-explained/graph1_full.svg" alt="First graph part" style="width: 30%;" />
  <img src="assets/posts/lstm-explained/preactivation_graph.svg" alt="First part closeup" style="width: 40%;" />
</div>

<p>Next, we have to apply a nonlinearity, but while all the gates use the sigmoid,
we will use a tanh for the input preactivation. Because of this, we will place two <code class="highlighter-rouge">nn.Narrow</code>
modules, which will select appropriate parts of the preactivation vector.</p>

<figure class="highlight"><pre><code class="language-lua" data-lang="lua"><span class="c1">-- gates</span>
<span class="kd">local</span> <span class="n">pre_sigmoid_chunk</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Narrow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">rnn_size</span><span class="p">)(</span><span class="n">preactivations</span><span class="p">)</span>
<span class="kd">local</span> <span class="n">all_gates</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sigmoid</span><span class="p">()(</span><span class="n">pre_sigmoid_chunk</span><span class="p">)</span>

<span class="c1">-- input</span>
<span class="kd">local</span> <span class="n">in_chunk</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Narrow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">rnn_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">)(</span><span class="n">preactivations</span><span class="p">)</span>
<span class="kd">local</span> <span class="n">in_transform</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">()(</span><span class="n">in_chunk</span><span class="p">)</span></code></pre></figure>

<p>After the nonlinearities we have to place a couple more <code class="highlighter-rouge">nn.Narrow</code>s and we have the gates done!</p>

<figure class="highlight"><pre><code class="language-lua" data-lang="lua"><span class="kd">local</span> <span class="n">in_gate</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Narrow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">)(</span><span class="n">all_gates</span><span class="p">)</span>
<span class="kd">local</span> <span class="n">forget_gate</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Narrow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">rnn_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">)(</span><span class="n">all_gates</span><span class="p">)</span>
<span class="kd">local</span> <span class="n">out_gate</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Narrow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">rnn_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">)(</span><span class="n">all_gates</span><span class="p">)</span></code></pre></figure>

<div class="images">
  <img src="assets/posts/lstm-explained/graph2_full.svg" alt="Second graph part" style="width: 30%;" />
  <img src="assets/posts/lstm-explained/gates.svg" alt="Second part closeup" style="width: 40%;" />
</div>

<h3 id="cell-and-hidden-state">Cell and hidden state</h3>

<p>Having computed the gate values we can now calculate the current cell state. All that’s required are just
two <code class="highlighter-rouge">nn.CMulTable</code> modules (one for \(f \cdot c_{t-1}^{l}\) and one for \(i \cdot x\)),
and a <code class="highlighter-rouge">nn.CAddTable</code> to sum them up to a current cell state.</p>

<figure class="highlight"><pre><code class="language-lua" data-lang="lua"><span class="c1">-- previous cell state contribution</span>
<span class="kd">local</span> <span class="n">c_forget</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CMulTable</span><span class="p">()({</span><span class="n">forget_gate</span><span class="p">,</span> <span class="n">prev_c</span><span class="p">})</span>
<span class="c1">-- input contribution</span>
<span class="kd">local</span> <span class="n">c_input</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CMulTable</span><span class="p">()({</span><span class="n">in_gate</span><span class="p">,</span> <span class="n">in_transform</span><span class="p">})</span>
<span class="c1">-- next cell state</span>
<span class="kd">local</span> <span class="n">next_c</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CAddTable</span><span class="p">()({</span>
  <span class="n">c_forget</span><span class="p">,</span>
  <span class="n">c_input</span>
<span class="p">})</span></code></pre></figure>

<p>It’s finally time to implement hidden state calculation. It’s the simplest part, because it just
involves applying tanh to current cell state (<code class="highlighter-rouge">nn.Tanh</code>) and multiplying it with an output gate
(<code class="highlighter-rouge">nn.CMulTable</code>).</p>

<figure class="highlight"><pre><code class="language-lua" data-lang="lua"><span class="kd">local</span> <span class="n">c_transform</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">()(</span><span class="n">next_c</span><span class="p">)</span>
<span class="kd">local</span> <span class="n">next_h</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CMulTable</span><span class="p">()({</span><span class="n">out_gate</span><span class="p">,</span> <span class="n">c_transform</span><span class="p">})</span></code></pre></figure>

<div class="images">
  <img src="assets/posts/lstm-explained/graph3_full.svg" alt="Third graph part" style="width: 30%;" />
  <img src="assets/posts/lstm-explained/state_calculation.svg" alt="Third part closeup" style="width: 40%;" />
</div>

<h3 id="defining-the-module">Defining the module</h3>

<p>Now, if you want to export the whole graph as a standalone module you can wrap it like that:</p>

<figure class="highlight"><pre><code class="language-lua" data-lang="lua"><span class="c1">-- module outputs</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="p">{}</span>
<span class="nb">table.insert</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">next_c</span><span class="p">)</span>
<span class="nb">table.insert</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">next_h</span><span class="p">)</span>

<span class="c1">-- packs the graph into a convenient module with standard API (:forward(), :backward())</span>
<span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">gModule</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span></code></pre></figure>

<h3 id="examples">Examples</h3>

<p>LSTM layer implementation is available <a href="/assets/posts/lstm-explained/LSTM.lua">here</a>.
You can use it like that:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">th&gt; </span>LSTM <span class="o">=</span> require <span class="s1">'LSTM.lua'</span>
                                                                      <span class="o">[</span>0.0224s]
<span class="gp">th&gt; </span>layer <span class="o">=</span> LSTM.create<span class="o">(</span>3, 2<span class="o">)</span>
                                                                      <span class="o">[</span>0.0019s]
<span class="gp">th&gt; </span>layer:forward<span class="o">({</span>torch.randn<span class="o">(</span>1,3<span class="o">)</span>, torch.randn<span class="o">(</span>1,2<span class="o">)</span>, torch.randn<span class="o">(</span>1,2<span class="o">)})</span>
<span class="o">{</span>
  1 : DoubleTensor - size: 1x2
  2 : DoubleTensor - size: 1x2
<span class="o">}</span>
                                                                      <span class="o">[</span>0.0005s]</code></pre></figure>

<p>To make a multi-layer LSTM network you can forward subsequent layers in a for loop,
taking <code class="highlighter-rouge">next_h</code> from previous layer as next layer’s input. You can check <a href="/assets/posts/lstm-explained/multilayer.lua">this example</a>.</p>

<h3 id="training">Training</h3>

<p>If you’re interested please leave a comment and I’ll try to expand this post!</p>

<h2 id="thats-it">That’s it!</h2>

<p>That’s it. It’s quite easy to implement any RNN when you understand how to deal with the hidden state.
After connecting several layers just put a regular MLP on top and connect it to last
layer’s hidden state and you’re done!</p>

<p>Here are some nice papers on RNNs if you’re interested:</p>

<ul>
  <li><a href="http://arxiv.org/abs/1506.02078">Visualizing and Understanding Recurrent Networks</a></li>
  <li><a href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">An Empirical Exploration of Recurrent Network Architectures</a></li>
  <li><a href="http://arxiv.org/abs/1409.2329">Recurrent Neural Network Regularization</a></li>
  <li><a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sequence to Sequence Learning with Neural Networks</a></li>
</ul>


    <div class="bdsharebuttonbox">
        <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
        <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
        <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
        <a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博"></a>
        <a href="#" class="bds_renren" data-cmd="renren" title="分享到人人网"></a>
        <a href="#" class="bds_mail" data-cmd="mail" title="分享到邮件分享"></a>
        <a href="#" class="bds_more" data-cmd="more"></a>
    </div>
    <nav class="article-previous fn-clear">
        
        
    </nav>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES * * */
      var disqus_shortname = 'andrechang67';

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>

                </div>

                <div class="aside">
                    <div class="aside-contact">
                        <h4 class="title">About me</h4>
                        <div class="det fn-clear">
                            <div class="det-image">
                                <img src="/images/header.jpg" />
                            </div>
                            <div class="det-text">
                                <p>Ola</p>
                            </div>
                        </div>
                    </div>

                    <div class="aside-item">
                        <h4 class="title">Recent Posts</h4>
                        <ul class="list">
                            
                                <li><a href="/computer_vision/2016/03/03/example.html" title="fgsfgsf" rel="bookmark">fgsfgsf</a></li>
                            
                        </ul>
                    </div>

                    <div class="aside-item">
                        <h4 class="title">Links</h4>
                        <ul class="list">
                            
                                
                            
                                
                            
                        </ul>
                    </div>
                </div>
            </div>
        </div>        
        
        <div class="foot">
            <div class="footer">
                <p>A blog template forked from <a href="https://github.com/zJiaJun/zJiaJun.github.io" target="_blank">zJiaJun</a>. Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a>.</p>
            </div>
        </div>

        <script type="text/javascript" src="http://ajax.useso.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
        
        
        
        <!-- mathjax -->
        <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
        <!-- Search in CSE home -->
        <script>
        (function() {
            var cx = '003287793477459687163:n59nxpgejxy';
            var gcse = document.createElement('script');
            gcse.type = 'text/javascript';
            gcse.async = true;
            gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
                '//cse.google.com/cse.js?cx=' + cx;
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(gcse, s);
        })();
        </script>
        <gcse:search></gcse:search>

        <!-- jekyll-table-of-contents -->
        
        <!-- <script src="/javascript/jquery-2.1.4.min.js" type="text/javascript"></script> -->
        <script src="/javascript/toc.js" type="text/javascript"></script>
        <!--
        <link rel="stylesheet" href="/css/bootstrap.min.css">
        <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
         -->

        <!-- Script to display table of content -->
        <script type="text/javascript">
        $(document).ready(function() {
            $('#toc').toc({ showEffect: 'slideDown' });
        }); </script>
        
    </body>
</html>